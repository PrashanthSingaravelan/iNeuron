{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we have seen cases of different types of data and different ways of processing them to obtain important information out of them, in this section we will talk about one of the most sought upon data in today’s time. \n",
    "Customer Information is the backbone for most of the renowned companies in 21st century. Let’s take example of Google. How do you suppose Google is providing so many services for free? What profit does it make by giving you free services? Well it’s your information that google sells to different companies by analyzing your searches and makes profit through it. The moment you search for a product on google, you will start seeing the product recommendation on every website. \n",
    "This is the most basic usage of text analytics, product recommendation to customers by analyzing their searches. Similarly, many companies analyze the positive or negative reviews given by customers and try to predict the customer behavior. \n",
    "There is a wealth of such unstructured data present such as emails, google searches, online surveys, twitter, online reviews etc. which can be processed using text analysis. Many key information about people, customers can be derived by processing the unstructured text and analyzing. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP)\n",
    "\n",
    "\n",
    "NLP is a part of Artificial Intelligence, developed for the machine to understand human language. The ultimate goal of NLP is to read, understand and make valuable conclusion of human language. It is a very tough job to do as human language has a lot of variation in terms of language, pronunciation etc. Although, in recent times there has been a major breakthrough in the field of NLP. \n",
    "\n",
    "Siri and Alexa are one such example of uses of NLP.\n",
    "\n",
    "\n",
    "We will use NLP for text analytics.\n",
    "\n",
    "\n",
    "There many libraries available for NLP in python. we will focus on the two most important one's :\n",
    "\n",
    "* Natural Languange Tool Kit (NLTK)\n",
    "* Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into text analytics using NLTK or Spacy. Let's understand about some important terminologies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is a process of breaking down a given paragraph of text into a list of sentence or words. When paragraph is broken down into list of sentences, it is called sentence tokenization.\n",
    "Similarly, if the sentences are further broken down into list of words, it is known as Word tokenization.\n",
    "\n",
    "Let's understand this with an example. Below is a given paragraph, let's see how tokenization works on it:\n",
    "\n",
    "\"India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia. It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.\"\n",
    "\n",
    "* Sentence Tokenize:\n",
    "\n",
    "    ['India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia.',\n",
    "    \n",
    "  'It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world.',\n",
    "  \n",
    "  'Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land      borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east.',\n",
    "  \n",
    "  'In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.']\n",
    "\n",
    "\n",
    "* Word tokenize:\n",
    "\n",
    "['India', '(', 'Hindi', ':', 'Bhārat', ')', ',', 'officially', 'the', 'Republic', 'of', 'India', ',', 'is', 'a', 'country', 'in', 'South',\n",
    " 'Asia', '.', 'It', 'is', 'the', 'seventh-largest', 'country', 'by', 'area', ',', 'the', 'second-most', 'populous', 'country', ',', 'and',\n",
    " 'the', 'most', 'populous', 'democracy', 'in', 'the', 'world', '.', 'Bounded', 'by', 'the', 'Indian',\n",
    " 'Ocean',\n",
    " 'on',\n",
    " 'the',\n",
    " 'south',\n",
    " ',',\n",
    " 'the',\n",
    " 'Arabian',\n",
    " 'Sea',\n",
    " 'on',\n",
    " 'the',\n",
    " 'southwest',\n",
    " ',',\n",
    " 'and',\n",
    " 'the',\n",
    " 'Bay',\n",
    " 'of',\n",
    " 'Bengal',\n",
    " 'on',\n",
    " 'the',\n",
    " 'southeast',\n",
    " ',',\n",
    " 'it',\n",
    " 'shares',\n",
    " 'land',\n",
    " 'borders',\n",
    " 'with',\n",
    " 'Pakistan',\n",
    " 'to',\n",
    " 'the',\n",
    " 'west',\n",
    " ';',\n",
    " 'China',\n",
    " ',',\n",
    " 'Nepal',\n",
    " ',',\n",
    " 'and',\n",
    " 'Bhutan',\n",
    " 'to',\n",
    " 'the',\n",
    " 'north',\n",
    " ';',\n",
    " 'and',\n",
    " 'Bangladesh',\n",
    " 'and',\n",
    " 'Myanmar',\n",
    " 'to',\n",
    " 'the',\n",
    " 'east',\n",
    " '.',\n",
    " 'In',\n",
    " 'the',\n",
    " 'Indian',\n",
    " 'Ocean',\n",
    " ',',\n",
    " 'India',\n",
    " 'is',\n",
    " 'in',\n",
    " 'the',\n",
    " 'vicinity',\n",
    " 'of',\n",
    " 'Sri',\n",
    " 'Lanka',\n",
    " 'and',\n",
    " 'the',\n",
    " 'Maldives',\n",
    " ';',\n",
    " 'its',\n",
    " 'Andaman',\n",
    " 'and',\n",
    " 'Nicobar',\n",
    " 'Islands',\n",
    " 'share',\n",
    " 'a',\n",
    " 'maritime',\n",
    " 'border',\n",
    " 'with',\n",
    " 'Thailand',\n",
    " 'and',\n",
    " 'Indonesia',\n",
    " '.']\n",
    "\n",
    "\n",
    "Hope this example clears up the concept of tokenization. We will understand why it is done when we will dive into text analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1 --> India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia. \\n2 --> It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world.\\n3 --> Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. \\n4 --> In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.\"\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1 --> India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia. \n",
    "2 --> It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world.\n",
    "3 --> Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. \n",
    "4 --> In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Prashanth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing using NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia.',\n",
       " 'It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world.',\n",
       " 'Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east.',\n",
       " 'In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia. It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.\"\n",
    "\n",
    "nltk.sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## wherever a full stop,it is trying to split up the sentences\n",
    "len(nltk.sent_tokenize(data)) ## Number of senteces --> 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia',\n",
       " ' It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world',\n",
       " ' Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east',\n",
       " ' In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia',\n",
       " '']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can also do by following conventional approach\n",
    "data.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India',\n",
       " '(',\n",
       " 'Hindi',\n",
       " ':',\n",
       " 'Bhārat',\n",
       " ')',\n",
       " ',',\n",
       " 'officially',\n",
       " 'the',\n",
       " 'Republic',\n",
       " 'of',\n",
       " 'India',\n",
       " ',',\n",
       " 'is',\n",
       " 'a',\n",
       " 'country',\n",
       " 'in',\n",
       " 'South',\n",
       " 'Asia',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'the',\n",
       " 'seventh-largest',\n",
       " 'country',\n",
       " 'by',\n",
       " 'area',\n",
       " ',',\n",
       " 'the',\n",
       " 'second-most',\n",
       " 'populous',\n",
       " 'country',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'most',\n",
       " 'populous',\n",
       " 'democracy',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'Bounded',\n",
       " 'by',\n",
       " 'the',\n",
       " 'Indian',\n",
       " 'Ocean',\n",
       " 'on',\n",
       " 'the',\n",
       " 'south',\n",
       " ',',\n",
       " 'the',\n",
       " 'Arabian',\n",
       " 'Sea',\n",
       " 'on',\n",
       " 'the',\n",
       " 'southwest',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Bay',\n",
       " 'of',\n",
       " 'Bengal',\n",
       " 'on',\n",
       " 'the',\n",
       " 'southeast',\n",
       " ',',\n",
       " 'it',\n",
       " 'shares',\n",
       " 'land',\n",
       " 'borders',\n",
       " 'with',\n",
       " 'Pakistan',\n",
       " 'to',\n",
       " 'the',\n",
       " 'west',\n",
       " ';',\n",
       " 'China',\n",
       " ',',\n",
       " 'Nepal',\n",
       " ',',\n",
       " 'and',\n",
       " 'Bhutan',\n",
       " 'to',\n",
       " 'the',\n",
       " 'north',\n",
       " ';',\n",
       " 'and',\n",
       " 'Bangladesh',\n",
       " 'and',\n",
       " 'Myanmar',\n",
       " 'to',\n",
       " 'the',\n",
       " 'east',\n",
       " '.',\n",
       " 'In',\n",
       " 'the',\n",
       " 'Indian',\n",
       " 'Ocean',\n",
       " ',',\n",
       " 'India',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " 'vicinity',\n",
       " 'of',\n",
       " 'Sri',\n",
       " 'Lanka',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Maldives',\n",
       " ';',\n",
       " 'its',\n",
       " 'Andaman',\n",
       " 'and',\n",
       " 'Nicobar',\n",
       " 'Islands',\n",
       " 'share',\n",
       " 'a',\n",
       " 'maritime',\n",
       " 'border',\n",
       " 'with',\n",
       " 'Thailand',\n",
       " 'and',\n",
       " 'Indonesia',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tags and Chunking\n",
    "\n",
    "* Parts Of Speech Tagging(POS tags)\n",
    "\n",
    "As the name suggests, it is a method of tagging individual words on the basis of it's parts of speech.\n",
    "\n",
    "Wikipedia definition : Part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech,based on both its definition and its context i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.\n",
    "\n",
    "There are 9 parts of speech in grammars, but in NLP there are more than 9 POS tags based on different set of rules, such as:\n",
    "\n",
    "* NN noun, singular 'table'\n",
    "* NNS noun plural 'tables'\n",
    "* NNP proper noun, singular \n",
    "* NNPS proper noun, plural \n",
    "\n",
    "There are 4 types of division for noun only. Similarly, there are multiple divisions for other part of speeches.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Prashanth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('see', 'VB'),\n",
       " ('an', 'DT'),\n",
       " ('example', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('POS', 'NNP'),\n",
       " ('tagging', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data =' We will see an example of POS tagging.'\n",
    "pos = nltk.pos_tag(nltk.word_tokenize(data))\n",
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Chunking \n",
    "\n",
    "After using parts of speech, Chunking can be used to make data more structured by giving a specific set of rules. Chunking is also known as shallow parser. \n",
    "Let's understand more about chunking by following example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  We/PRP\n",
      "  will/MD\n",
      "  see/VB\n",
      "  an/DT\n",
      "  (MN example/NN)\n",
      "  of/IN\n",
      "  (MN POS/NNP tagging/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "data =' We will see an example of POS tagging.'\n",
    "\n",
    "pos = nltk.pos_tag(nltk.word_tokenize(data))\n",
    "\n",
    "# now once the POS tag has been done. Let's say we want to further structure data such that Nouns are\n",
    "# categorized under one specific node defined by us :\n",
    "\n",
    "my_node = \"MN: {<NNP>*<NN>}\"\n",
    "\n",
    "chunk  =nltk.RegexpParser(my_node)\n",
    "result = chunk.parse(pos)\n",
    "print(result)\n",
    "result.draw()    # It will draw the pattern graphically which can be seen in Noun Phrase chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Graphical representation\n",
    "\n",
    "<img src=\"chunk.PNG\">\n",
    "\n",
    "\n",
    "We can see that both NN and NNP are now categorised into \"MN\" (as the given tag_name). \n",
    "\n",
    "So, whenever we need to categorise different tags into one tag, we can use chunking for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "Stop words are such words which are very common in occurrence such as ‘a’,’an’,’the’, ‘at’ etc. We ignore such words during the preprocessing part since they do not give any important information and would just take additional space. We can make our custom list of stop words as well if we want. Different libraries have different stop words list. Let’s see the stop words list for NLTK:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Prashanth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('french')\n",
    "print(stop_words)  ## These french words are not-all going to provide use-ful info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)  ## These english words are not-all going to provide use-ful info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We also have punctuations which we can ignore from our set of words just like stopwords.\n",
    "\n",
    "import string\n",
    "\n",
    "punct = string.punctuation\n",
    "punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India', 'Hindi', 'Bhārat', 'officially', 'Republic', 'India', 'country', 'South', 'Asia', 'It', 'seventh-largest', 'country', 'area', 'second-most', 'populous', 'country', 'populous', 'democracy', 'world', 'Bounded', 'Indian', 'Ocean', 'south', 'Arabian', 'Sea', 'southwest', 'Bay', 'Bengal', 'southeast', 'shares', 'land', 'borders', 'Pakistan', 'west', 'China', 'Nepal', 'Bhutan', 'north', 'Bangladesh', 'Myanmar', 'east', 'In', 'Indian', 'Ocean', 'India', 'vicinity', 'Sri', 'Lanka', 'Maldives', 'Andaman', 'Nicobar', 'Islands', 'share', 'maritime', 'border', 'Thailand', 'Indonesia']\n"
     ]
    }
   ],
   "source": [
    "# Let's word tokenize the given sample after we remove the stopwords and punctuation. \n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "punct = string.punctuation\n",
    "\n",
    "data = \"India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia. It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.\"\n",
    "clean_data =[]\n",
    "for word in nltk.word_tokenize(data):\n",
    "    if (word not in punct) and  (word not in stop_words):\n",
    "            clean_data.append(word)\n",
    "            \n",
    "print(clean_data)  ## Removed all the punctuation and unwanted words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Great!! Our data looks so much cleaner now after removing stop words and punctuation.**\n",
    "\n",
    "Hope, this clears up why we should remove stop words and punctuation before processing our data.\n",
    "\n",
    "Let's see pos tagging for our cleaned data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('India', 'NNP'), ('Hindi', 'NNP'), ('Bhārat', 'NNP'), ('officially', 'RB'), ('Republic', 'NNP'), ('India', 'NNP'), ('country', 'NN'), ('South', 'NNP'), ('Asia', 'IN'), ('It', 'PRP'), ('seventh-largest', 'JJ'), ('country', 'NN'), ('area', 'NN'), ('second-most', 'RB'), ('populous', 'JJ'), ('country', 'NN'), ('populous', 'JJ'), ('democracy', 'NN'), ('world', 'NN'), ('Bounded', 'NNP'), ('Indian', 'JJ'), ('Ocean', 'NNP'), ('south', 'NN'), ('Arabian', 'NNP'), ('Sea', 'NNP'), ('southwest', 'JJS'), ('Bay', 'NNP'), ('Bengal', 'NNP'), ('southeast', 'NN'), ('shares', 'NNS'), ('land', 'VBP'), ('borders', 'NNS'), ('Pakistan', 'NNP'), ('west', 'JJS'), ('China', 'NNP'), ('Nepal', 'NNP'), ('Bhutan', 'NNP'), ('north', 'JJ'), ('Bangladesh', 'NNP'), ('Myanmar', 'NNP'), ('east', 'NN'), ('In', 'IN'), ('Indian', 'JJ'), ('Ocean', 'NNP'), ('India', 'NNP'), ('vicinity', 'NN'), ('Sri', 'NNP'), ('Lanka', 'NNP'), ('Maldives', 'NNP'), ('Andaman', 'NNP'), ('Nicobar', 'NNP'), ('Islands', 'NNP'), ('share', 'NN'), ('maritime', 'JJ'), ('border', 'NN'), ('Thailand', 'NNP'), ('Indonesia', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(clean_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Many words that are used in a sentence are not always used in their basic form but are used as per the rules of grammar e.g.\n",
    "\n",
    "running ---> run (base word)\n",
    "\n",
    "runs    ---> run (base word)    \n",
    "\n",
    "ran     ---> run (base word)\n",
    "\n",
    "Although, the underlying meaning will be same but form of the base word changes to preserve the correct grammatical meaning.\n",
    "\n",
    "Stemming and Lemmatization are basically used to bring such words to their basic forms, so that the words with same base are treated as same words rather than treated differently.\n",
    "\n",
    "The only difference in Stemming and Lemmatization is the way in which they change the word to its base form.\n",
    "\n",
    "* Stemming\n",
    "\n",
    "Stemming means mapping a group of words to the same stem by removing prefixes or suffixes without giving any value to the “grammatical meaning” of the stem formed after the process.\n",
    "\n",
    "e.g.\n",
    "\n",
    "computation --> comput\n",
    "\n",
    "computer --> comput \n",
    "\n",
    "hobbies --> hobbi\n",
    "\n",
    "We can see that stemming tries to bring the word back to their base word but the base word may or may not have correct grammatical meanings.\n",
    "\n",
    "There are typically two types of stemmers available in NLTK package.\n",
    "1)\tPorter Stemmer \n",
    "2)\tLancaster Stemmer\n",
    "\n",
    "Let’s see how to use both of them: \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer ,LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter stemmer\n",
      "hobbi\n",
      "hobbi\n",
      "comput\n",
      "comput\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "print('Porter stemmer')\n",
    "print(porter.stem(\"hobby\"))\n",
    "print(porter.stem(\"hobbies\"))\n",
    "print(porter.stem(\"computer\"))\n",
    "print(porter.stem(\"computation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lancaster stemmer\n",
      "hobby\n",
      "hobby\n",
      "comput\n",
      "comput\n"
     ]
    }
   ],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "\n",
    "print('lancaster stemmer')\n",
    "print(lancaster.stem(\"hobby\"))\n",
    "print(lancaster.stem(\"hobbies\"))\n",
    "print(lancaster.stem(\"computer\"))\n",
    "print(porter.stem(\"computation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowball stemmer\n",
      "hobbi\n",
      "hobbi\n",
      "comput\n",
      "comput\n"
     ]
    }
   ],
   "source": [
    "Snowball = SnowballStemmer(\"english\")\n",
    "\n",
    "print('Snowball stemmer')\n",
    "print(Snowball.stem(\"hobby\"))\n",
    "print(Snowball.stem(\"hobbies\"))\n",
    "print(Snowball.stem(\"computer\"))\n",
    "print(Snowball.stem(\"computation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was go to the offic on my bike when i saw a car pass by hit the tree .\n",
      "i was going to the off on my bik when i saw a car pass by hit the tre .\n",
      "i wa go to the offic on my bike when i saw a car pass by hit the tree .\n"
     ]
    }
   ],
   "source": [
    "sent = \"I was going to the office on my bike when i saw a car passing by hit the tree.\"\n",
    "token = list(nltk.word_tokenize(sent))\n",
    "for stemmer in (Snowball, lancaster, porter):\n",
    "    stemm = [stemmer.stem(t) for t in token]\n",
    "    print(\" \".join(stemm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lancaster algorithm is faster than porter but it is more complex.\n",
    "Porter stemmer is the oldest algorithm present and was the most popular to use.\n",
    "\n",
    "Snowball stemmer, also known as  porter2, is the updated version of the Porter stemmer and is currently the most popular stemming algorithm.\n",
    "\n",
    "Snowball stemmer is available for multiple languages as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "print(porter.stem(\"running\"))\n",
    "print(porter.stem(\"runs\"))\n",
    "print(porter.stem(\"ran\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lemmatization\n",
    "\n",
    "\n",
    "Lemmatization also does the same thing as stemming and try to bring a word to its base form, but unlike stemming it do keep in account the actual meaning of the base word i.e. the base word belongs to any specific language. The ‘base word’ is known as ‘Lemma’.\n",
    "\n",
    "We use WordNet Lemmatizer for Lemmatization in nltk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Prashanth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "run\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "print(lemma.lemmatize('running'))\n",
    "print(lemma.lemmatize('runs'))\n",
    "print(lemma.lemmatize('ran'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see the lemma has changed for the words with same base. \n",
    "\n",
    "This is because, we haven’t given any context to the Lemmatizer.\n",
    "\n",
    "Generally, it is given by passing the POS tags for the words in a sentence.\n",
    "e.g.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "## Lematizer a word based on the verb\n",
    "print(lemma.lemmatize('running',pos='v'))\n",
    "print(lemma.lemmatize('runs',pos='v'))\n",
    "print(lemma.lemmatize('ran',pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizer is very complex and takes a lot of time to calculate.\n",
    "\n",
    "So, it should only when the real meaning of words or the context is necessary for processing, else stemming should be preferred.\n",
    "\n",
    "It completely depends on the type of problem you are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition(NER)\n",
    "\n",
    "In chunking, we read that we can set rules to keep different POS tags under one sinlge user defined tag. One such form of chunking in NLP is known as Named Entity Recognition.\n",
    "\n",
    "In NER, we try to group entities like people, places, countries, things etc. together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Prashanth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Prashanth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE India/NNP)\n",
      "  ,/,\n",
      "  officially/RB\n",
      "  the/DT\n",
      "  (ORGANIZATION Republic/NNP)\n",
      "  of/IN\n",
      "  (GPE India/NNP)\n",
      "  ,/,\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  country/NN\n",
      "  in/IN\n",
      "  (GPE South/NNP Asia/NNP)\n",
      "  ./.)\n",
      "\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "sent = \"India, officially the Republic of India, is a country in South Asia.\"\n",
    "\n",
    "words = nltk.word_tokenize(sent)\n",
    "pos_tag = nltk.pos_tag(words)\n",
    "namedEntity = nltk.ne_chunk(pos_tag)\n",
    "print(namedEntity)\n",
    "print(\"\\n\",namedEntity.draw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph \n",
    "\n",
    "<img src=\"NER1.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"NER2.PNG\">\n",
    "\n",
    "\n",
    "image source= nltk.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of NER, we can select any particular category from a given word document or sentence.\n",
    "Suppose we need all the names mentioned in a document, we can use NER and select the words with tag \"Person\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing\n",
    "\n",
    "Parsing is the process of determining the structure of a given a text on the basis of a given grammatical rule.\n",
    "\n",
    "e.g.\n",
    "* Divide a sentence as Noun_phrase and Verb_phrase.\n",
    "* Break Noun_phrase further in Proper noun, determiner, noun\n",
    "* Break Verb_phrase in verb, noun_phrase \n",
    "\n",
    "These are set of grammatical rules we will use to parse a given text.\n",
    "\n",
    "text = \" Ram ate a mango.\"\n",
    "\n",
    "* Proper_noun = Ram\n",
    "* Noun = mango\n",
    "* determiner =  a\n",
    "* verb = ate\n",
    "\n",
    "Once you pass the above mentioned set of grammatical rules in a parser. It will break the given \"text\" on the basis of it and output like this:\n",
    "\n",
    "Noun_Phrase( Proper_Noun Ram) , Verb_Phrase(verb (ate), noun_phrase( determiner(a), noun(mango))\n",
    "\n",
    "Such type of parser is called a **Recursive Parser**.\n",
    "\n",
    "There are many different types of Parsers available in Nltk library. Each one has a different usecase and can be used on the basis of requirement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf (Term frequency–Inverse document frequency)\n",
    "\n",
    "Wikipedia definition:  ” Tf-Idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The Tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. Tf–idf is one of the most popular term-weighting schemes today.”\n",
    "\n",
    "\n",
    "### Term Frequency\n",
    "It is simply the frequency in which a word appears in a document in comparison to the total number words in the document. Mathematically given as:\n",
    "\n",
    "Term frequency = (Number of times a word appears in the document) / (Total number of words in the document)\n",
    "\n",
    "### Inverse Document Frequency\n",
    "\n",
    "Term frequency has a disadvantage that it tends to give higher weights to words with higher frequency. In such cases words like ‘a’, ‘the’, ‘in’, ’of’ etc. appears more in the documents than other regular words. Thus, more important words are wrongly given lower weights as their frequency is less.\n",
    " To tackle this problem IDF was introduced. IDF decreases the weights of such high frequency terms and increases the weight of terms with rare occurrence. Mathematically it is given as:\n",
    " \n",
    "Inverse Document Frequency = log [(Number of documents)/(Number of documents the word appears in)]   \n",
    "\n",
    "note: [log has base 2]\n",
    "\n",
    "\n",
    "*Tf-Idf Score = Term frequency * Inverse Document Frequency*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand more with an example:\n",
    "\n",
    "Doc 1: This is an example.\n",
    "\n",
    "Doc 2: We will see how it works.\n",
    "\n",
    "Doc 3: IDF can be confusing.\n",
    "\n",
    "\n",
    "\n",
    "<img src= \"tfidf.PNG\">\n",
    "\n",
    "In the above table, we have calculated the term frequency as well as inverse document frequency of each of the words present in the 3 documents given. \n",
    "\n",
    "Now, let's calculate the tf-idf score for each term. Since, words of one document is not present in another document, we will have tf-idf value 0 for them e.g. words of doc1 will have 0 tf-idf for doc2 and doc3.\n",
    "\n",
    "<img src= \"tfidf2.PNG\">\n",
    "\n",
    "Great, hope this example must have cleared how Tf-Idf works. \n",
    "\n",
    "let's see the python implementation for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'apple', 'arm', 'basics', 'in', 'is', 'my', 'name', 'prashanth', 'teaching', 'works']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfid = TfidfVectorizer(smooth_idf=False)  ## smooth_idf --> normalize the complete data-set\n",
    "\n",
    "doc= [\"My name is Prashanth\",\n",
    "      \"Prashanth works in Apple\",\n",
    "      \"I am teaching ARM basics\"]\n",
    "\n",
    "doc_vector = tfid.fit_transform(doc)\n",
    "print(tfid.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>am</th>\n",
       "      <th>apple</th>\n",
       "      <th>arm</th>\n",
       "      <th>basics</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>my</th>\n",
       "      <th>name</th>\n",
       "      <th>prashanth</th>\n",
       "      <th>teaching</th>\n",
       "      <th>works</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538498</td>\n",
       "      <td>0.538498</td>\n",
       "      <td>0.538498</td>\n",
       "      <td>0.360638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.360638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    am     apple  arm  basics        in        is        my      name  \\\n",
       "0  0.0  0.000000  0.0     0.0  0.000000  0.538498  0.538498  0.538498   \n",
       "1  0.0  0.538498  0.0     0.0  0.538498  0.000000  0.000000  0.000000   \n",
       "2  0.5  0.000000  0.5     0.5  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "   prashanth  teaching     works  \n",
       "0   0.360638       0.0  0.000000  \n",
       "1   0.360638       0.0  0.538498  \n",
       "2   0.000000       0.5  0.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.DataFrame(doc_vector.todense(),columns=tfid.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.3606383263504801\n",
      "  (0, 5)\t0.5384979101064753\n",
      "  (0, 7)\t0.5384979101064753\n",
      "  (0, 6)\t0.5384979101064753\n",
      "  (1, 1)\t0.5384979101064753\n",
      "  (1, 4)\t0.5384979101064753\n",
      "  (1, 10)\t0.5384979101064753\n",
      "  (1, 8)\t0.3606383263504801\n",
      "  (2, 3)\t0.5\n",
      "  (2, 2)\t0.5\n",
      "  (2, 9)\t0.5\n",
      "  (2, 0)\t0.5\n"
     ]
    }
   ],
   "source": [
    "print(doc_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we are using the same data set as we used while doing manual calculation, the results are different than what we got.\n",
    "\n",
    "This is because sklearn package have some modifications done to the formula to avoid complete avoidance of terms as well as to counter dividing by zero. \n",
    "\n",
    "You can know more by going through the official doumentation of sklearn as below:\n",
    "\n",
    "\"\n",
    "   *The formula that is used to compute the tf-idf for a term t of a document d\n",
    "    in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is\n",
    "    computed as idf(t) = log [ n / df(t) ] + 1 (if ``smooth_idf=False``), where\n",
    "    n is the total number of documents in the document set and df(t) is the\n",
    "    document frequency of t; the document frequency is the number of documents\n",
    "    in the document set that contain the term t. The effect of adding \"1\" to\n",
    "    the idf in the equation above is that terms with zero idf, i.e., terms\n",
    "    that occur in all documents in a training set, will not be entirely\n",
    "    ignored.\n",
    "    (Note that the idf formula above differs from the standard textbook\n",
    "    notation that defines the idf as\n",
    "    idf(t) = log [ n / (df(t) + 1) ]).\n",
    "    If ``smooth_idf=True`` (the default), the constant \"1\" is added to the\n",
    "    numerator and denominator of the idf as if an extra document was seen\n",
    "    containing every term in the collection exactly once, which prevents\n",
    "    zero divisions: idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfid = TfidfVectorizer()\n",
    "\n",
    "doc= [\"My name is Prashanth\",\n",
    "      \"Prashanth works in Apple\",\n",
    "      \"I am teaching ARM basics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'apple', 'arm', 'basics', 'in', 'is', 'my', 'name', 'prashanth', 'teaching', 'works']\n"
     ]
    }
   ],
   "source": [
    "doc_vector = tfid.fit_transform(doc)\n",
    "print(tfid.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>am</th>\n",
       "      <th>apple</th>\n",
       "      <th>arm</th>\n",
       "      <th>basics</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>my</th>\n",
       "      <th>name</th>\n",
       "      <th>prashanth</th>\n",
       "      <th>teaching</th>\n",
       "      <th>works</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.528635</td>\n",
       "      <td>0.528635</td>\n",
       "      <td>0.528635</td>\n",
       "      <td>0.40204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.528635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.528635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.40204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.528635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    am     apple  arm  basics        in        is        my      name  \\\n",
       "0  0.0  0.000000  0.0     0.0  0.000000  0.528635  0.528635  0.528635   \n",
       "1  0.0  0.528635  0.0     0.0  0.528635  0.000000  0.000000  0.000000   \n",
       "2  0.5  0.000000  0.5     0.5  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "   prashanth  teaching     works  \n",
       "0    0.40204       0.0  0.000000  \n",
       "1    0.40204       0.0  0.528635  \n",
       "2    0.00000       0.5  0.000000  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.DataFrame(doc_vector.todense(),columns=tfid.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.4020402441612698\n",
      "  (0, 5)\t0.5286346066596935\n",
      "  (0, 7)\t0.5286346066596935\n",
      "  (0, 6)\t0.5286346066596935\n",
      "  (1, 1)\t0.5286346066596935\n",
      "  (1, 4)\t0.5286346066596935\n",
      "  (1, 10)\t0.5286346066596935\n",
      "  (1, 8)\t0.4020402441612698\n",
      "  (2, 3)\t0.5\n",
      "  (2, 2)\t0.5\n",
      "  (2, 9)\t0.5\n",
      "  (2, 0)\t0.5\n"
     ]
    }
   ],
   "source": [
    "df= pd.DataFrame(doc_vector.todense(),columns=tfid.get_feature_names())\n",
    "print(doc_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BllipParser',\n",
       " 'BottomUpChartParser',\n",
       " 'BottomUpLeftCornerChartParser',\n",
       " 'BottomUpProbabilisticChartParser',\n",
       " 'ChartParser',\n",
       " 'CoreNLPDependencyParser',\n",
       " 'CoreNLPParser',\n",
       " 'DependencyEvaluator',\n",
       " 'DependencyGraph',\n",
       " 'EarleyChartParser',\n",
       " 'FeatureBottomUpChartParser',\n",
       " 'FeatureBottomUpLeftCornerChartParser',\n",
       " 'FeatureChartParser',\n",
       " 'FeatureEarleyChartParser',\n",
       " 'FeatureIncrementalBottomUpChartParser',\n",
       " 'FeatureIncrementalBottomUpLeftCornerChartParser',\n",
       " 'FeatureIncrementalChartParser',\n",
       " 'FeatureIncrementalTopDownChartParser',\n",
       " 'FeatureTopDownChartParser',\n",
       " 'IncrementalBottomUpChartParser',\n",
       " 'IncrementalBottomUpLeftCornerChartParser',\n",
       " 'IncrementalChartParser',\n",
       " 'IncrementalLeftCornerChartParser',\n",
       " 'IncrementalTopDownChartParser',\n",
       " 'InsideChartParser',\n",
       " 'LeftCornerChartParser',\n",
       " 'LongestChartParser',\n",
       " 'MaltParser',\n",
       " 'NaiveBayesDependencyScorer',\n",
       " 'NonprojectiveDependencyParser',\n",
       " 'ParserI',\n",
       " 'ProbabilisticNonprojectiveParser',\n",
       " 'ProbabilisticProjectiveDependencyParser',\n",
       " 'ProjectiveDependencyParser',\n",
       " 'RandomChartParser',\n",
       " 'RecursiveDescentParser',\n",
       " 'ShiftReduceParser',\n",
       " 'SteppingChartParser',\n",
       " 'SteppingRecursiveDescentParser',\n",
       " 'SteppingShiftReduceParser',\n",
       " 'TestGrammar',\n",
       " 'TopDownChartParser',\n",
       " 'TransitionParser',\n",
       " 'UnsortedChartParser',\n",
       " 'ViterbiParser',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'api',\n",
       " 'bllip',\n",
       " 'chart',\n",
       " 'corenlp',\n",
       " 'dependencygraph',\n",
       " 'earleychart',\n",
       " 'evaluate',\n",
       " 'extract_test_sentences',\n",
       " 'featurechart',\n",
       " 'load_parser',\n",
       " 'malt',\n",
       " 'nonprojectivedependencyparser',\n",
       " 'pchart',\n",
       " 'projectivedependencyparser',\n",
       " 'recursivedescent',\n",
       " 'shiftreduce',\n",
       " 'transitionparser',\n",
       " 'util',\n",
       " 'viterbi']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nltk.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "  S   -> NP VP\n",
    "  VP  -> V NP | V NP PP\n",
    "  PP  -> P NP\n",
    "  V   -> \"saw\" | \"slept\" | \"walked\"\n",
    "  NP  -> \"Rahul\" | \"Anjali\" | Det N | Det N PP\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "  N   -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "  P   -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Rahul)\n",
      "  (VP (V saw) (NP Anjali) (PP (P with) (NP (Det a) (N dog)))))\n"
     ]
    }
   ],
   "source": [
    "sent = \"Rahul saw Anjali with a dog\".split()\n",
    "parser = nltk.RecursiveDescentParser(grammar)\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree) \n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the graph after parsing ::\n",
    "\n",
    "<img src=\"parse.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectorization (Word Embedding)\n",
    "\n",
    "Word vectorization is the process of mapping words to a set of real numbers or vectors. This is done to process the given words using machine learning techniques and extract relevant information from them such that it can be used in further predicting words. Vectorization is done by comparing a given word to the corpus(collection) of the available words.\n",
    "There are many different methods used for vectorizing a given set of words. let's see some of the mosed popular ones:\n",
    "\n",
    "### Count Vectorizer\n",
    "\n",
    "Count vectorizer uses two of the following models as the base to vectorize the given words on the basis of frequency of words.\n",
    "\n",
    "#### Bag of Words Model\n",
    "BOW model is used in NLP to represent the given text/sentence/document as a collection (bag) of words without giving any importance to grammar or the occurrence order of the words. It keeps the account of frequency of the words in the text document, which can be used as features in many models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s understand this with an example:\n",
    "\n",
    "Text1 = “I went to have a cup of coffee but I ended up having lunch with her.”\n",
    "\n",
    "Text2 = “I don’t understand, what is the problem here?”\n",
    "\n",
    "BOW1 = {I :2, went : 1, to : 1,have : 1, a : 1, cup: 1, of :1, coffee : 1, but :1, ended : 1, up :1,having : 1, with :1, her :1}\n",
    "\n",
    "BOW2 = {I : 1, don’t : 1, understand:1, what : 1 , is :1, the : 1, problem : 1, here : 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW is mainly used for feature selection. The above dictionary is converted as a list with only the frequency terms there and on that basis, weights are given to the most occurring terms. But the “stop words” are the most frequent words that appears in raw document. Thus, having a word with high frequency count doesn’t mean that the word is as important. To resolve this problem, “Tf-idf” was introduced. We will discuss about it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram model\n",
    "\n",
    "As discussed in bag of words model, BOW model doesn’t keep the sequence of words in a given text, only the frequency of words matters. It doesn’t take into account the context of the given sentence, or care for grammatical rules such as verb is following a proper noun in the given text.n-gram model is used in such cases to keep the context of the given text intact. N-gram is the sequence of n words from a given text/document.\n",
    "\n",
    "When, n= 1, we call it a “unigram”.\n",
    "\n",
    "             n=2, it is called a “bigram”. \n",
    "             \n",
    "             n=3, it is called a “trigram”.\n",
    "And so on.\n",
    "\n",
    "Let’s understand this with an example:\n",
    "\n",
    "Text1 = “I went to have a cup of coffee but I ended up having lunch with her.”\n",
    "\n",
    "* Unigram \n",
    "\n",
    "[I, went, to, have, a, cup, of, coffee, but, I, ended, up, having, lunch, with, her]\n",
    "\n",
    "* Bi-gram\n",
    "\n",
    "[I went], [went to],[to have],[have a],[a cup],[cup f],[of coffee],[coffee but],[but I],[I ended],[ended up],\n",
    "[up having],[having lunch],[lunch with],[with her]\n",
    "\n",
    "* Tri-gram\n",
    "\n",
    "[I went to], [went to have], [to have a], [have a cup],[ a cup of], [cup of coffee],[ of coffee but],[ coffee but I],[but I ended],[I ended up],[ended up having],[up having lunch],[having lunch with],[lunch with her].\n",
    "\n",
    "Note: We can clearly see that BOW model is nothing but n-gram model when n=1.\n",
    "\n",
    "Skip-grams\n",
    "\n",
    "Skip grams are type of n-grams where the words are not necessarily in the same order as are in the given text i.e. some words can be skipped. \n",
    "Example:\n",
    "\n",
    "Text2 = “I don’t understand, what is the problem here?”\n",
    "\n",
    "1-skip 2-grams (we have to make 2-gram while skipping 1 word)\n",
    "\n",
    "[I understand, don’t what, understand is, what the, is problem, the here].\n",
    "\n",
    "\n",
    "Let's see the implementation of Count vectorizer in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = [\"This is an example of n-gram!\"]\n",
    "vect1 = CountVectorizer(ngram_range=(1,1))\n",
    "vect1.fit_transform(string)\n",
    "\n",
    "vect2 = CountVectorizer(ngram_range=(2,2))\n",
    "vect2.fit_transform(string)\n",
    "\n",
    "vect3 = CountVectorizer(ngram_range=(3,3))\n",
    "vect3.fit_transform(string)\n",
    "\n",
    "vect4 = CountVectorizer(ngram_range=(4,4))\n",
    "vect4.fit_transform(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram  : ['an', 'example', 'gram', 'is', 'of', 'this']\n",
      "2-gram  : ['an example', 'example of', 'is an', 'of gram', 'this is']\n",
      "3-gram  : ['an example of', 'example of gram', 'is an example', 'this is an']\n",
      "4-gram  : ['an example of gram', 'is an example of', 'this is an example']\n"
     ]
    }
   ],
   "source": [
    "print(\"1-gram  :\",vect1.get_feature_names())\n",
    "print(\"2-gram  :\",vect2.get_feature_names())\n",
    "print(\"3-gram  :\",vect3.get_feature_names())\n",
    "print(\"4-gram  :\",vect4.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words : ['an', 'bag', 'example', 'is', 'of', 'this', 'words']\n"
     ]
    }
   ],
   "source": [
    "## Bag Of Words\n",
    "string = [\"This is an example of bag of words!\"]\n",
    "vect1 = CountVectorizer()\n",
    "vect1.fit_transform(string)\n",
    "print(\"bag of words :\",vect1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
